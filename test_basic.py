#!/usr/bin/env python3
"""
Basic test script for OuiComply MCP Server.

This script tests the basic functionality without requiring API keys.
"""

import asyncio
import json
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from tools.document_ai import DocumentAIService, DocumentAnalysisRequest, ComplianceIssue
from tools.compliance_engine import ComplianceEngine, ComplianceReport, ComplianceStatus, RiskLevel
from tools.memory_integration import LeChatMemoryService


def test_models():
    """Test Pydantic models."""
    print("üß™ Testing Pydantic Models...")
    print("=" * 30)
    
    try:
        # Test DocumentAnalysisRequest
        request = DocumentAnalysisRequest(
            document_content="test content",
            compliance_frameworks=["gdpr"],
            analysis_depth="basic"
        )
        print("‚úÖ DocumentAnalysisRequest model works")
        
        # Test ComplianceIssue
        issue = ComplianceIssue(
            issue_id="test_1",
            severity="high",
            category="data_protection",
            description="Test issue",
            recommendation="Fix it",
            framework="gdpr",
            confidence=0.8
        )
        print("‚úÖ ComplianceIssue model works")
        
        # Test ComplianceReport
        report = ComplianceReport(
            report_id="test_report",
            document_id="test_doc",
            generated_at="2024-01-01T00:00:00Z",
            overall_status=ComplianceStatus.COMPLIANT,
            risk_level=RiskLevel.LOW,
            risk_score=0.2,
            frameworks_analyzed=["gdpr"],
            summary="Test summary",
            issues=[issue],
            missing_clauses=["clause1"],
            mitigation_actions=[],
            recommendations=["rec1"],
            metadata={"test": "value"}
        )
        print("‚úÖ ComplianceReport model works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Model test failed: {str(e)}")
        return False


def test_compliance_frameworks():
    """Test compliance framework loading."""
    print("\nüìã Testing Compliance Frameworks...")
    print("=" * 35)
    
    try:
        # Create a mock config
        class MockConfig:
            mistral_api_key = "test_key"
        
        # Test framework loading
        service = DocumentAIService()
        service.config = MockConfig()
        
        frameworks = service._load_compliance_frameworks()
        
        print(f"‚úÖ Loaded {len(frameworks)} compliance frameworks:")
        for name, framework in frameworks.items():
            print(f"   - {name.upper()}: {framework['name']}")
            print(f"     Required clauses: {len(framework['required_clauses'])}")
            print(f"     Risk indicators: {len(framework['risk_indicators'])}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Compliance frameworks test failed: {str(e)}")
        return False


def test_risk_calculation():
    """Test risk score calculation."""
    print("\nüìä Testing Risk Score Calculation...")
    print("=" * 35)
    
    try:
        service = DocumentAIService()
        
        # Test with no issues
        score_no_issues = service._calculate_risk_score([])
        print(f"‚úÖ No issues risk score: {score_no_issues}")
        
        # Test with various issues
        issues = [
            ComplianceIssue(
                issue_id="1",
                severity="critical",
                category="test",
                description="Critical issue",
                recommendation="Fix it",
                framework="gdpr",
                confidence=0.9
            ),
            ComplianceIssue(
                issue_id="2",
                severity="low",
                category="test",
                description="Low issue",
                recommendation="Monitor",
                framework="gdpr",
                confidence=0.5
            )
        ]
        
        score_with_issues = service._calculate_risk_score(issues)
        print(f"‚úÖ Mixed issues risk score: {score_with_issues:.2f}")
        
        # Test individual severities
        for severity in ["low", "medium", "high", "critical"]:
            test_issue = ComplianceIssue(
                issue_id=f"test_{severity}",
                severity=severity,
                category="test",
                description=f"{severity} issue",
                recommendation="Fix it",
                framework="gdpr",
                confidence=0.8
            )
            score = service._calculate_risk_score([test_issue])
            print(f"   {severity.upper()}: {score:.2f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Risk calculation test failed: {str(e)}")
        return False


def test_compliance_engine():
    """Test compliance engine functionality."""
    print("\nüîß Testing Compliance Engine...")
    print("=" * 30)
    
    try:
        engine = ComplianceEngine()
        
        # Test compliance status determination
        print("Testing compliance status determination...")
        
        # Mock analysis result for compliant document
        from tools.document_ai import DocumentAnalysisResult
        compliant_result = DocumentAnalysisResult(
            document_id="test_doc",
            document_type="text/plain",
            analysis_timestamp="2024-01-01T00:00:00Z",
            compliance_issues=[],
            risk_score=0.2,
            missing_clauses=[],
            recommendations=[],
            metadata={}
        )
        
        status = engine._determine_compliance_status(compliant_result)
        print(f"‚úÖ Compliant document status: {status.value}")
        
        # Test risk level determination
        risk_levels = [
            (0.1, "low"),
            (0.4, "medium"),
            (0.7, "high"),
            (0.9, "critical")
        ]
        
        for score, expected in risk_levels:
            level = engine._determine_risk_level(score)
            print(f"‚úÖ Risk score {score} -> {level.value} (expected {expected})")
        
        # Test mitigation action generation
        issues = [
            ComplianceIssue(
                issue_id="1",
                severity="high",
                category="data_protection",
                description="Missing privacy notice",
                recommendation="Add privacy notice",
                framework="gdpr",
                confidence=0.8
            )
        ]
        
        actions = engine._generate_mitigation_actions(issues)
        print(f"‚úÖ Generated {len(actions)} mitigation actions")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Compliance engine test failed: {str(e)}")
        return False


def test_document_processing():
    """Test document processing functions."""
    print("\nüìÑ Testing Document Processing...")
    print("=" * 32)
    
    try:
        service = DocumentAIService()
        
        # Test document type determination
        print("Testing document type determination...")
        
        # Test with provided type
        doc_type = service._determine_document_type("application/pdf", b"test content")
        print(f"‚úÖ Provided type: {doc_type}")
        
        # Test with auto-detection (fallback)
        doc_type = service._determine_document_type(None, b"test content")
        print(f"‚úÖ Auto-detected type: {doc_type}")
        
        # Test analysis prompt generation
        print("Testing analysis prompt generation...")
        framework_def = {
            "name": "Test Framework",
            "required_clauses": ["clause1", "clause2"],
            "risk_indicators": ["risk1", "risk2"]
        }
        
        prompt = service._generate_analysis_prompt(framework_def, "comprehensive", "doc123")
        print(f"‚úÖ Generated prompt ({len(prompt)} characters)")
        print(f"   Contains framework name: {'Test Framework' in prompt}")
        print(f"   Contains required clauses: {'clause1' in prompt}")
        
        # Test AI response parsing
        print("Testing AI response parsing...")
        
        # Test valid JSON response
        valid_response = '''
        {
            "issues": [
                {
                    "issue_id": "test1",
                    "severity": "high",
                    "category": "test",
                    "description": "Test issue",
                    "location": "Section 1",
                    "recommendation": "Fix it",
                    "confidence": 0.8
                }
            ],
            "missing_clauses": ["clause1"],
            "recommendations": ["rec1"]
        }
        '''
        
        parsed = service._parse_ai_response(valid_response, "gdpr")
        print(f"‚úÖ Valid JSON parsed: {len(parsed['issues'])} issues")
        
        # Test invalid response fallback
        invalid_response = "This is not valid JSON"
        parsed = service._parse_ai_response(invalid_response, "gdpr")
        print(f"‚úÖ Invalid response fallback: {len(parsed['issues'])} issues")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Document processing test failed: {str(e)}")
        return False


def test_mcp_server_initialization():
    """Test MCP server initialization."""
    print("\nüöÄ Testing MCP Server Initialization...")
    print("=" * 40)
    
    try:
        from mcp_server import OuiComplyMCPServer
        
        # Create mock config
        class MockConfig:
            mistral_api_key = "test_key"
            server_name = "test-server"
            server_version = "1.0.0"
        
        # Initialize server
        server = OuiComplyMCPServer()
        server.config = MockConfig()
        
        print("‚úÖ MCP Server initialized successfully")
        print(f"   Server name: {server.config.server_name}")
        print(f"   Server version: {server.config.server_version}")
        print(f"   Reports cache: {len(server._reports_cache)} items")
        
        return True
        
    except Exception as e:
        print(f"‚ùå MCP Server initialization test failed: {str(e)}")
        return False


def test_file_operations():
    """Test file operations with sample document."""
    print("\nüìÅ Testing File Operations...")
    print("=" * 28)
    
    try:
        # Check if sample document exists
        sample_file = Path("sample_privacy_policy.txt")
        if not sample_file.exists():
            print("‚ùå Sample document not found")
            return False
        
        # Read sample document
        content = sample_file.read_text()
        print(f"‚úÖ Sample document read: {len(content)} characters")
        
        # Test document processing
        service = DocumentAIService()
        
        # Mock the async function for testing
        async def test_process():
            return await service._process_document_content(sample_file)
        
        # This would normally be async, but we'll test the sync parts
        print("‚úÖ File operations test completed")
        
        return True
        
    except Exception as e:
        print(f"‚ùå File operations test failed: {str(e)}")
        return False


def main():
    """Main test function."""
    print("üß™ OuiComply Basic System Test")
    print("=" * 50)
    print("This test runs without requiring API keys")
    print()
    
    tests = [
        ("Pydantic Models", test_models),
        ("Compliance Frameworks", test_compliance_frameworks),
        ("Risk Calculation", test_risk_calculation),
        ("Compliance Engine", test_compliance_engine),
        ("Document Processing", test_document_processing),
        ("MCP Server Initialization", test_mcp_server_initialization),
        ("File Operations", test_file_operations),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nüîç Running {test_name} test...")
        try:
            if test_func():
                print(f"‚úÖ {test_name} test passed")
                passed += 1
            else:
                print(f"‚ùå {test_name} test failed")
        except Exception as e:
            print(f"üí• {test_name} test crashed: {str(e)}")
    
    print("\n" + "=" * 50)
    print(f"üìä Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All basic tests passed! System is ready for API testing.")
        print("\nüöÄ Next steps:")
        print("   1. Set your MISTRAL_KEY in .env file")
        print("   2. Run: python test_system.py")
        print("   3. Test with real documents")
        return True
    else:
        print("‚ùå Some tests failed. Please check the errors above.")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
