#!/usr/bin/env python3
"""
Test the MCP Server properly with async methods and real API calls.
"""

import asyncio
import json
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.mcp_server import OuiComplyMCPServer


async def test_mcp_server_proper():
    """Test the MCP server with proper async calls."""
    
    print("ğŸš€ Testing OuiComply MCP Server with Real API Calls")
    print("=" * 60)
    
    try:
        # Initialize the MCP server
        server = OuiComplyMCPServer()
        print("âœ… MCP Server initialized successfully!")
        
        # Test 1: List available tools
        print("\nğŸ“‹ Test 1: Listing Available Tools")
        print("-" * 30)
        
        # Get the tools handler
        tools_handler = server._setup_handlers()['tools']['list']
        tools = await tools_handler()
        
        print(f"âœ… Found {len(tools)} tools:")
        for tool in tools:
            print(f"   ğŸ”§ {tool.name}: {tool.description}")
        
        # Test 2: Test decompose_task tool
        print("\nğŸ” Test 2: Testing decompose_task Tool")
        print("-" * 30)
        
        decompose_args = {
            "query": "Analyze this service agreement for GDPR compliance issues",
            "team_context": "Legal Team"
        }
        
        # Get the call tool handler
        call_tool_handler = server._setup_handlers()['tools']['call']
        decompose_response = await call_tool_handler("decompose_task", decompose_args)
        
        print("âœ… decompose_task executed successfully!")
        if decompose_response:
            response_text = decompose_response[0].text
            response_data = json.loads(response_text)
            print(f"   ğŸ“„ Document: {response_data.get('document_name', 'N/A')}")
            print(f"   ğŸ‘¥ Team: {response_data.get('team_context', 'N/A')}")
            print(f"   ğŸ” Query Type: {response_data.get('query_type', 'N/A')}")
        
        # Test 3: Test analyze_with_memory tool
        print("\nğŸ§  Test 3: Testing analyze_with_memory Tool")
        print("-" * 30)
        
        analyze_args = {
            "document_content": "Test service agreement content for compliance analysis",
            "document_type": "service_agreement",
            "frameworks": ["gdpr"],
            "team_id": "legal_team"
        }
        
        analyze_response = await call_tool_handler("analyze_with_memory", analyze_args)
        
        print("âœ… analyze_with_memory executed successfully!")
        if analyze_response:
            response_text = analyze_response[0].text
            response_data = json.loads(response_text)
            print(f"   ğŸ“Š Analysis completed: {response_data.get('status', 'N/A')}")
            print(f"   âš ï¸  Issues found: {len(response_data.get('compliance_issues', []))}")
            print(f"   ğŸ“ˆ Risk score: {response_data.get('risk_score', 0):.2f}")
        
        # Test 4: Test generate_structured_report tool
        print("\nğŸ“Š Test 4: Testing generate_structured_report Tool")
        print("-" * 30)
        
        report_args = {
            "analysis_results": {
                "compliance_issues": [{"severity": "high", "description": "Test issue"}],
                "risk_score": 0.75,
                "missing_clauses": ["Test clause"]
            },
            "team_context": "Legal Team"
        }
        
        report_response = await call_tool_handler("generate_structured_report", report_args)
        
        print("âœ… generate_structured_report executed successfully!")
        if report_response:
            response_text = report_response[0].text
            response_data = json.loads(response_text)
            print(f"   ğŸ“ Report generated: {len(response_data.get('formatted_response', ''))} characters")
            print(f"   ğŸ’¡ Learning prompt: {len(response_data.get('learning_prompt', ''))} characters")
        
        # Test 5: Test generate_automation_prompts tool
        print("\nğŸ¤– Test 5: Testing generate_automation_prompts Tool")
        print("-" * 30)
        
        automation_args = {
            "analysis_results": {
                "document_type": "service_agreement",
                "frameworks": ["gdpr"],
                "issues_found": 2,
                "risk_score": 0.75
            },
            "team_context": "Legal Team",
            "priority": "high"
        }
        
        automation_response = await call_tool_handler("generate_automation_prompts", automation_args)
        
        print("âœ… generate_automation_prompts executed successfully!")
        if automation_response:
            response_text = automation_response[0].text
            response_data = json.loads(response_text)
            print(f"   ğŸ“‹ Linear tasks: {len(response_data.get('tasks', []))}")
            print(f"   ğŸ“§ Slack notifications: {len(response_data.get('notifications', []))}")
            print(f"   ğŸ› GitHub issues: {len(response_data.get('issues', []))}")
        
        # Test 6: Test get_team_memory tool
        print("\nğŸ§  Test 6: Testing get_team_memory Tool")
        print("-" * 30)
        
        memory_args = {
            "team_id": "legal_team"
        }
        
        memory_response = await call_tool_handler("get_team_memory", memory_args)
        
        print("âœ… get_team_memory executed successfully!")
        if memory_response:
            response_text = memory_response[0].text
            response_data = json.loads(response_text)
            print(f"   ğŸ‘¥ Team: {response_data.get('team_id', 'N/A')}")
            print(f"   ğŸ“š Compliance rules: {len(response_data.get('compliance_rules', []))}")
            print(f"   âš ï¸  Pitfall patterns: {len(response_data.get('pitfall_patterns', []))}")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ALL MCP SERVER API TESTS PASSED!")
        print("=" * 60)
        
        print("\nğŸ“ˆ Test Results Summary:")
        print("   âœ… MCP Server Initialization: Working")
        print("   âœ… Tool Listing: Working")
        print("   âœ… decompose_task: Working")
        print("   âœ… analyze_with_memory: Working")
        print("   âœ… generate_structured_report: Working")
        print("   âœ… generate_automation_prompts: Working")
        print("   âœ… get_team_memory: Working")
        
        print("\nğŸš€ MCP Server is fully functional with real API calls!")
        print("   Ready for hackathon submission! ğŸ†")
        
        return True
        
    except Exception as e:
        print(f"âŒ MCP Server test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_mcp_server_proper())
    if success:
        print("\nâœ… All MCP Server tests passed!")
    else:
        print("\nâŒ MCP Server tests failed!")
        sys.exit(1)
